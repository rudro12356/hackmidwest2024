{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore.config\n",
    "import json\n",
    "\n",
    "#load env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIASULCY7EJWI7FGYYF\n",
      "v9DvNanlHy8AkfuV/lVI1W8NcbhRg6UasBdN9WUJ\n",
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv('AWS_ACCESS_KEY_ID'))\n",
    "print(os.getenv('AWS_SECRET_ACCESS_KEY'))\n",
    "print(os.getenv('AWS_DEFAULT_REGION'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup bedrock\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-west-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conversation(model_id, system_prompts, messages):\n",
    "    \"\"\"\n",
    "    Sends messages to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_prompts (JSON) : The system prompts for the model to use.\n",
    "        messages (JSON) : The messages to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Generating message with model {model_id}\")\n",
    "\n",
    "    # Inference parameters to use.\n",
    "    temperature = 0.5\n",
    "\n",
    "    # Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": temperature}\n",
    "    # Additional inference parameters to use.\n",
    "    # top_k = 200\n",
    "    # additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        # additionalModelRequestFields=additional_model_fields,\n",
    "    )\n",
    "\n",
    "    # Log token usage.\n",
    "    token_usage = response[\"usage\"]\n",
    "    print(f\"Input tokens: {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens: {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens: {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "\n",
    "    text_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "    return text_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"meta.llama3-1-70b-instruct-v1:0\",\n",
    "    \"meta.llama3-1-405b-instruct-v1:0\",\n",
    "    \"meta.llama3-1-8b-instruct-v1:0\",\n",
    "    \"mistral.mistral-large-2402-v1:0\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    \"\"\"\n",
    "    Function to summarize text using a generative AI model.\n",
    "    \"\"\"\n",
    "\n",
    "    model_id = \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "    # Setup the system prompts and messages to send to the model.\n",
    "    system_prompts = [\n",
    "        {\"text\": \"You are an app that creates summaries of text in 50 words or less.\"}\n",
    "    ]\n",
    "    message_1 = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": f\"Summarize the following text: {text}.\"}],\n",
    "    }\n",
    "\n",
    "    messages = [message_1]\n",
    "\n",
    "    result = generate_conversation(model_id, system_prompts, messages)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    \"\"\"\n",
    "    Function to return a JSON object of sentiment from a given text.\n",
    "    \"\"\"\n",
    "\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    # Setup the system prompts and messages to send to the model.\n",
    "    system_prompts = [\n",
    "        {\n",
    "            \"text\": \"You are a bot that takes text and returns a JSON object of sentiment analysis.\"\n",
    "        }\n",
    "    ]\n",
    "    message_1 = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": f\"{text}\"}],\n",
    "    }\n",
    "\n",
    "    messages = [message_1]\n",
    "\n",
    "    result = generate_conversation(model_id, system_prompts, messages)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_qa(question, text):\n",
    "    \"\"\"\n",
    "    Function to perform a Q&A operation based on the provided text.\n",
    "    \"\"\"\n",
    "\n",
    "    model_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "    # Setup the system prompts and messages to send to the model.\n",
    "    system_prompts = [\n",
    "        {\n",
    "            \"text\": f\"Given the following text, answer the question. If the answer is not in the text, 'say you do not know'. Here is the text: {text}\"\n",
    "        }\n",
    "    ]\n",
    "    message_1 = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": f\"{question}\"}],\n",
    "    }\n",
    "\n",
    "    messages = [message_1]\n",
    "\n",
    "    result = generate_conversation(model_id, system_prompts, messages)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summarization Example ===\n",
      "Generating message with model meta.llama3-1-70b-instruct-v1:0\n",
      "Input tokens: 208\n",
      "Output tokens: 67\n",
      "Total tokens: 275\n",
      "Stop reason: end_turn\n",
      "Summary:\n",
      "\n",
      "\n",
      "Here is a summary of the text in 50 words or less:\n",
      "\n",
      "Amazon Bedrock is a managed service offering high-performing foundation models from leading AI companies via a single API. It enables experimentation, customization, and deployment of generative AI applications with security, privacy, and responsible AI capabilities, all without requiring infrastructure management.\n",
      "\n",
      "=== Sentiment Analysis Example ===\n",
      "Generating message with model anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Input tokens: 204\n",
      "Output tokens: 146\n",
      "Total tokens: 350\n",
      "Stop reason: end_turn\n",
      "Sentiment_Analysis JSON:\n",
      "Here is the sentiment analysis for the given text:\n",
      "\n",
      "{\n",
      "  \"sentiment\": \"positive\",\n",
      "  \"confidence\": 0.9,\n",
      "  \"analysis\": \"The text is overwhelmingly positive in its description of Amazon Bedrock, highlighting its capabilities, features, and advantages. It portrays Bedrock as a powerful and convenient service for building generative AI applications, emphasizing its ease of use, security, privacy, and responsible AI features. The text also highlights Bedrock's integration with leading AI companies and models, as well as its serverless nature and compatibility with existing AWS services. Overall, the language used is highly promotional and enthusiastic about the service.\"\n",
      "}\n",
      "\n",
      "=== Q&A Example ===\n",
      "How many companies have models in Amazon Bedrock?\n",
      "Generating message with model mistral.mistral-large-2402-v1:0\n",
      "Input tokens: 227\n",
      "Output tokens: 44\n",
      "Total tokens: 271\n",
      "Stop reason: end_turn\n",
      "Answer: According to the provided text, there are six companies that have models in Amazon Bedrock. These include AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon itself.\n",
      "\n",
      "Can Amazon Bedrock support RAG?\n",
      "Generating message with model mistral.mistral-large-2402-v1:0\n",
      "Input tokens: 225\n",
      "Output tokens: 62\n",
      "Total tokens: 287\n",
      "Stop reason: end_turn\n",
      "Answer: Yes, according to the text, Amazon Bedrock supports Retrieval Augmented Generation (RAG). It states that using Amazon Bedrock, you can privately customize foundation models with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG).\n",
      "\n",
      "When was Amazon Bedrock announced?\n",
      "Generating message with model mistral.mistral-large-2402-v1:0\n",
      "Input tokens: 224\n",
      "Output tokens: 14\n",
      "Total tokens: 238\n",
      "Stop reason: end_turn\n",
      "Answer: The text does not provide the announcement date for Amazon Bedrock.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Sample text for summarization\n",
    "    text = \"Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with\"\n",
    "\n",
    "    print(\"\\n=== Summarization Example ===\")\n",
    "    summary = summarize_text(text)\n",
    "    print(f\"Summary:\\n{summary}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(\"\\n=== Sentiment Analysis Example ===\")\n",
    "    sentiment_analysis_json = sentiment_analysis(text)\n",
    "    print(f\"Sentiment_Analysis JSON:\\n{sentiment_analysis_json}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(\"\\n=== Q&A Example ===\")\n",
    "\n",
    "    q1 = \"How many companies have models in Amazon Bedrock?\"\n",
    "    print(q1)\n",
    "    answer = perform_qa(q1, text)\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    q2 = \"Can Amazon Bedrock support RAG?\"\n",
    "    print(q2)\n",
    "    answer = perform_qa(q2, text)\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    q3 = \"When was Amazon Bedrock announced?\"\n",
    "    print(q3)\n",
    "    answer = perform_qa(q3, text)\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackmidwest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
